---
output: 
  pdf_document:
    number_sections: true
    includes:
      in_header: 00_header.tex
fontsize: 12pt
fig_caption: true
geometry: margin=1in
linestretch: 1.5
---

```{r setup, echo=FALSE, message=FALSE}
library(knitr)
library(tidyverse)
library(xtable)
library(MASS)

knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE, fig.height = 2)
theme_set(theme_bw(base_family = "serif"))

set.seed(305)
```

\setcounter{section}{6}
\setcounter{subsection}{3}
\setcounter{ex}{16}
\setcounter{df}{8}
\setcounter{page}{31}

## Inference for matched pairs and two-sample data

An important type of application of confidence interval estimation and significance testing is when we either have *paired data* or *two-sample* data.

### Matched pairs

Recall,

\vspace{1in}

Examples:

\vspace{2in}

One simple method of investigating the possibility of a consistent difference between paired data is to

1.
\vspace{.5in}
2.

\newpage

\begin{ex}[Fuel economy]
Twelve cars were equipped with radial tires and driven over a test course. Then the same twelve cars (with the same drivers) were equipped with regular belted tires and driven over the same course. After each run, the cars gas economy (in km/l) was measured. Using significance level $\alpha= 0.05$ and the method of critical values, test for a difference in fuel economy between the radial tires and belted tires. Construct a 95\% confidence interval for true mean difference due to tire type.
\end{ex}

```{r}
tires <- data.frame(car = 1:12,
                    radial = c(4.2, 4.7, 6.6, 7.0, 6.7, 4.5, 5.7, 6.0, 7.4, 4.9, 6.1, 5.2),
                    belted = c(4.1, 4.9, 6.2, 6.9, 6.8, 4.4, 5.7, 5.8, 6.9, 4.7, 6.0, 4.9))

kable(t(tires))
```

\newpage

\begin{ex}[End-cut router]
Consider the operation of an end-cut router in the manufacture of a company's wood product. Both a leading-edge and a trailing-edge measurement were made on each wooden piece to come off the router. Is the leading-edge measurement different from the trailing-edge measurement for a typical wood piece?  Do a hypothesis test at $\alpha= 0.05$ to find out. Make a two-sided 95\% confidence interval for the true mean of the difference between the measurements.
\end{ex}

```{r}
wood <- data.frame(piece = 1:5,
                   leading_edge = c(.168, .170, .165, .165, .170),
                   trailing_edge = c(.169, .168, .168, .168, .169))

kable(t(wood))
```

\newpage

### Two-sample data

Paired differences provide inference methods of a special kind for comparison. Methods that can be used to compare two means where two different *unrelated* samples will be discussed next.

Examples:

\vspace{2in}

Notation:

\newpage

#### Large samples ($n_1 \ge 25, n_2 \ge 25$)

The difference in sample means $\overline{x}_1 - \overline{x}_2$ is a natural statistic to use in comparing $\mu_1$ and $\mu_2$. 

If $\sigma_1$ and $\sigma_2$ are **known**, then Proposition 5.1 tells us

\vspace{.2in}

$\text{E}(\overline{X}_1 - \overline{X}_2) =$ 

\vspace{1in}

$\text{Var}(\overline{X}_1 - \overline{X}_2) =$

\vspace{1in}

If, in addition, $n_1$ and $n_2$ are large,

\newpage

So, if we want to test $\text{H}_0: \mu_1 - \mu_2 = \#$ with some alternative hypothesis, $\sigma_1$ and $\sigma_2$ are known, and $n_1 \ge 25, n_2 \ge 25$, then we use the statistic

\vspace{.5in}

$K =$

\vspace{.5in}
 
which has a $N(0,1)$ distribution if

1. $\text{H}_0$ is true
2. The sample 1 points are iid with mean $\mu_1$ and variance $\sigma^2_1$, and the sample 2 points are iid with mean $\mu_2$ and variance $\sigma^2_2$.

\vspace{.5in}

The confidence intervals (2-sided, 1-sided upper, and 1-sided lower, respectively) for $\mu_1-\mu_2$ are:

\newpage

If $\sigma_1$ and $\sigma_2$ are **unknown**, and $n_1 \ge 25, n_2 \ge 25$, then we use the statistic


\vspace{.5in}

$K =$

\vspace{.5in}
 
and confidence intervals (2-sided, 1-sided upper, and 1-sided lower, respectively) for $\mu_1-\mu_2$:


\newpage

\begin{ex}[Anchor bolts]
An experiment carried out to study various characteristics of anchor bolts resulted in 78 observations on shear strength (kip) of 3/8-in. diameter bolts and 88 observations on strength of 1/2-in. diameter bolts. Let Sample 1 be the 1/2 in diameter bolts and Sample 2 be the 3/8 indiameter bolts. Using a significance level of $\alpha= 0.01$, find out if the 1/2 in bolts are more than 2 kip stronger (in shear strength) than the 3/8 in bolts. Calculate and interpret the appropriate 99\% confidence interval to support the analysis.
\end{ex}
- $n1 =88, n2 =78$
- $\overline{x}_1 = 7.14, \overline{x}_2 =4.25$
- $s_1 =1.68,s_2 =1.3$

\newpage

#### Small samples

If $n_1 < 25$ or $n_2 < 25$, then we need some **other assumptions** to hold in order to complete inference on two-sample data.

\vspace{1in}

A test statistic to test $\text{H}_0:\mu_1-\mu_2= \#$ against some alternative is 
\vspace{.5in}
$K=$
\vspace{.5in}

Also assuming 
- $\text{H}_0$ is true, 
- The sample 1 points are iid $N(\mu_1,\sigma^2_1)$, the sample 2 points are iid $N(\mu_2,\sigma^2_2)$, 
- and the sample 1 points are independent of the sample 2 points.

Then
\vspace{.5in}
$K\sim$
\vspace{.5in}

$1-\alpha$ confidence intervals (2-sided, 1-sided upper, and 1-sided lower, respectively) for $\mu_1-\mu_2$ under these assumptions are of the form:

\newpage

\begin{ex}[Springs]
The data of W. Armstrong on spring lifetimes (appearing in the book by Cox and Oakes) not only concern spring longevity at a 950 N/$\text{mm}^2$ stress level but also longevity at a 900 N/$\text{mm}^2$ stress level. Let sample 1 be the 900 N/$\text{mm}^2$ stress group and sample 2 be the 950 N/$\text{mm}^2$ stress group. Letâ€™s do a hypothesis test to see if the sample 1 springs lasted significantly longer than the sample 2 springs.
\end{ex}

```{r, fig.cap="Normal plots of spring lifetimes under two different levels of stress."}
springs <- data.frame(stress_level = c(rep(950, 10), rep(900, 10)),
                      lifetimes = c(225, 171, 198, 189, 189, 135, 162, 135, 117, 162, 216, 162, 153, 216, 225, 216, 306, 225, 243, 189))

springs %>%
  group_by(stress_level) %>%
  summarise(lifetimes = paste(lifetimes, collapse = ", ")) %>%
  mutate(stress_level = paste(stress_level, "N/mm2 Stress")) %>%
  spread(stress_level, lifetimes) %>%
  kable()

springs %>%
  ggplot(aes(sample=lifetimes, colour = factor(stress_level), group = factor(stress_level))) +
  stat_qq() +
  scale_color_discrete("Stress level (N/mm2)")

```

\newpage

\newpage

\begin{ex}[Stopping distance]
Suppose $\mu_1$ and $\mu_2$ are true mean stopping distances (in meters) at 50 mph for cars of a certain type equipped with two different types of breaking systems. Suppose $n_1=n_2= 6$, $\overline{x}_1= 115.7$, $\overline{x}_2= 129.3$, $s_1=5.08$, and $s_2= 5.38$. Use significance level $\alpha = 0.01$ to test $\text{H}_0: \mu_1-\mu_2 =-10$ vs. $\text{H}_A:\mu_1-\mu_2 < -10$. Construct a 2-sided 99% confidence interval for the true difference in stopping distances.

\end{ex}

\newpage

## Prediction intervals

Methods of confidence interval estimation andd hypothesis testing concern the problem of reasoning from sample information to statements about underlying *parameters* of the data generation (such as $\mu$). 

Sometimes it is useful to not make a statement about a parameter value, but create bounds on other *individual values* generated by the process.

> How can we use out data $x_1, \dots, x_n$ to create an interval likely to contain one additional (as yet unobserved) value $x_{n+1}$ from the same data generating mechanism?

\vspace{.2in}

Let $X_1, \dots, X_n$ be iid Normal random variables with

\begin{align*}
\text{E}(X_i) &= \mu \text{ for all } i = 1, \dots, n \\
\text{Var}(X_i) &= \sigma^2 \text{ for all } i = 1, \dots, n
\end{align*}

Then,

\vspace{1in}

Let $X_{n+1}$ be an additional observation from the same data generating mechanism.

\newpage

$\text{E}(\overline{X}_n - X_{n+1}) =$

\vfill

$\text{Var}(\overline{X}_n - X_{n+1}) =$

\vfill

So,

\vspace{1in}

\newpage

Generally, $\sigma$ is unknown, so replace $\sigma$ by $s$, and

\vspace{2in}

Then, $1-\alpha$ **Prediction intervals** for $X_{n+1}$ are

\newpage
