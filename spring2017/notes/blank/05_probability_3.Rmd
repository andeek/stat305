---
output: 
  pdf_document:
    number_sections: true
    includes:
      in_header: 00_header.tex
fontsize: 12pt
fig_caption: true
geometry: margin=1in
linestretch: 1.5
---

```{r setup, echo=FALSE, message=FALSE}
library(knitr)
library(tidyverse)
library(xtable)
library(MASS)

knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE, fig.height = 2)
theme_set(theme_bw(base_family = "serif"))

set.seed(305)
```

\setcounter{section}{5}
\setcounter{subsection}{3}
\setcounter{ex}{31}
\setcounter{df}{20}
\setcounter{page}{53}

## Joint distributions and independence (discrete)

Most applications of probability to engineering statistics involve not one but several random variables. In some cases, the application is intrinsically multivariate.

\vspace{.2in}

\begin{ex}
Consider the assembly of a ring bearing with nominal inside diameter $1.00$ in. on a rod with nominal diameter $.99$ in. If
\begin{align*}
X &= \text{the ring bearing inside diameter} \\
Y &= \text{the rod diameter}
\end{align*}
One might be interested in
$$
P[\text{there is an interference in assembly}] = \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad
$$
\end{ex}

\vspace{.2in}

Even when a situation is univariate, samples larger than size $1$ are essentially always used in engineering applications. The $n$ data values in a sample are usually thought of as subject to chance and their simultaneous behavior must then be modeled.

\vfill

This is actually a very broad and difficult subject, we will only cover a brief introduction to the topic: **jointly discrete random variables**.

\newpage

### Joint distributions

For several discrete random variable, the device typically used to specify probabilities is a *joint probability function*. The two-variable version of this is defined.

\vspace{.2in}

\begin{df}
A \emph{joint probability function (joint pmf)} for discrete random variables $X$ and $Y$ is a nonnegative function $f(x, y)$, giving the probability that (simultaneously) $X$ takes the values $x$ and $Y$ takes the values $y$. That is,
$$
f(x, y) = P[X = x \text{ and } Y = y]
$$
\end{df}

\vspace{.2in}

Properties:

\begin{enumerate}
\itemsep .5in
\item
\item
\end{enumerate}

\vspace{.3in}

For the discrete case, it is useful to give $f(x,y)$ in a **table**.

\newpage

\begin{ex}[Two bolt torques, cont'd]
Recall the example of measure the bolt torques on the face plates of a heavy equipment component to the nearest integer. With

\begin{align*}
X &= \text{the next torque recorded for bolt }3\\
Y &= \text{the next torque recorded for bolt }4
\end{align*}
the joint probability function, $f(x,y)$, is

```{r, results='asis'}
bolt <- expand.grid(x = 11:20, y = 20:13)
bolt$f <- c(rep(0, 7), "2/34", "2/34", "1/34",
            rep(0, 6), "2/34", rep(0, 3),
            rep(0, 2), "1/34", "1/34", 0, 0, "1/34", "1/34", "1/34", 0,
            rep(0, 4), "2/34", "1/34", "1/34", "2/34", 0, 0,
            rep(0, 3), "1/34", "2/34", "2/34", 0, 0, "2/34", 0,
            rep("1/34", 2), rep(0, 2), "3/34", rep(0, 5),
            rep(0, 4), "1/34", 0, 0, "2/34", 0, 0,
            rep(0, 4), "1/34", rep(0, 5))

bolt %>%
  spread(x, f) %>%
  arrange(desc(y)) %>%
  rename(`y\\x` = y) %>%
  xtable() %>%
  print(include.rownames = FALSE, comment = FALSE)

```
\vspace{.2in}
$P[X = 18 \text{ and } Y = 17]$


\vspace{.4in}
$P[X = 14 \text{ and } Y = 19]$
\vspace{.4in}

By summing up certain values of $f(x, y)$, probabilities associated with $X$ and $Y$ with patterns of interest can be obtained.

\newpage

Consider:
$$
P[X \ge Y]
$$

```{r, results='asis'}
bolt %>% dplyr::select(-f) %>%
  mutate(g = "") %>%
  spread(x, g) %>%
  arrange(desc(y)) %>%
  rename(`y\\x` = y) %>%
  xtable(align = paste0("|", paste(rep("c", ncol(.) + 1), collapse = "|"), "|")) %>%
  print(include.rownames = FALSE, comment = FALSE, hline.after = -1:nrow(.))
```
$$
P[|X - Y| \le 1]
$$

```{r, results='asis'}
bolt %>% dplyr::select(-f) %>%
  mutate(g = "") %>%
  spread(x, g) %>%
  arrange(desc(y)) %>%
  rename(`y\\x` = y) %>%
  xtable(align = paste0("|", paste(rep("c", ncol(.) + 1), collapse = "|"), "|")) %>%
  print(include.rownames = FALSE, comment = FALSE, hline.after = -1:nrow(.))
```

$$
P[X = 17]
$$
```{r results='asis'}
bolt %>% dplyr::select(-f) %>%
  mutate(g = "") %>%
  spread(x, g) %>%
  arrange(desc(y)) %>%
  rename(`y\\x` = y) %>%
  xtable(align = paste0("|", paste(rep("c", ncol(.) + 1), collapse = "|"), "|")) %>%
  print(include.rownames = FALSE, comment = FALSE, hline.after = -1:nrow(.))

```

\end{ex}

### Maginal distributions

In a bivariate problem, once can add down columns in the (two-way) table of $f(x, y)$ to get values for the probability function of $X$, $f_X(x)$ and across rows in the same table to get values for the probability distribution of $Y$, $f_Y(y)$.

\begin{df}
The individual probability functions for discrete random variables $X$ and $Y$ with joint probability function $f(x, y)$ are called \emph{marginal probability functions}. They are obtained by summing $f(x, y)$ values over all possible values of the other variable.

\begin{align*}
f_X(x) &= \sum\limits_y f(x, y) \\
f_Y(y) &= \sum\limits_x f(x, y)
\end{align*}
\end{df}

\begin{ex}[Torques, cont'd]
Find the marginal probability functions for $X$ and $Y$ from the following joint pmf.


```{r, results='asis'}
bolt %>%
  spread(x, f) %>%
  arrange(desc(y)) %>%
  rename(`y\\x` = y) %>%
  xtable() %>%
  print(include.rownames = FALSE, comment = FALSE)

```
\end{ex}

\newpage

Getting marginal probability functions from joint probability functions begs the question whether the process can be reversed. **Can we find joint probability functions from marginal probability functions?**

\vspace{2in}


### Conditional distributions

When working with several random variables, it is often useful to think about what is expected of one of the variables, given the values assumed by all others.

\vspace{1in}

\begin{df}
For discrete random variables $X$ and $Y$ with joint probability function $f(x, y)$, the \emph{conditional probability function of $X$ given $Y = y$} is the function of $x$
$$
f_{X|Y}(x|y) = \frac{f(x, y)}{f_{Y}(y)} = \frac{f(x, y)}{\sum\limits_{x}f(x, y)}
$$
and the \emph{conditional probability function of $Y$ given $X = x$} is the function of $y$
$$
f_{Y|X}(y|x) = \frac{f(x, y)}{f_{X}(x)} = \frac{f(x, y)}{\sum\limits_{y}f(x, y)}.
$$
\end{df}

\newpage

\begin{ex}[Torque, cont'd]

For the torque example with the following joint distribution, find the following:
\begin{enumerate}
\item $f_{Y|X}(20|18)$
\item $f_{Y|X}(y|15)$
\item $f_{Y|X}(y|20)$
\item $f_{X|Y}(x|18)$
\end{enumerate}

```{r, results='asis'}
bolt %>%
  mutate(f = vapply(f, function(num) eval(parse(text = num)), numeric(1))*34) %>%
  spread(x, f) %>%
  arrange(desc(y)) %>%
  rename(`$y\\backslash x$` = y) %>%
  rbind(colSums(.)) %>%
  bind_cols(data.frame(rowSums(.[,-1]))) -> bolt_dsn

names(bolt_dsn)[ncol(bolt_dsn)] <- "$f_Y(y)$"

bolt_dsn_table <- bolt_dsn
bolt_dsn_table[, -1] <- apply(bolt_dsn[, -1], 2, function(col) paste0(col, "/34"))
bolt_dsn_table[nrow(bolt_dsn_table), 1] <- "$f_X(x)$"

bolt_dsn_table %>%
  xtable(align = paste0(paste(rep("c", ncol(.)), collapse = ""), "|c")) %>%
  print(include.rownames = FALSE, comment = FALSE, 
        sanitize.text.function = function(x){x},
        hline.after = c(-1, 0, nrow(.) - 1, nrow(.)))


```

\end{ex}

\newpage

### Independence

Recall the following joint distribution:

```{r, results='asis'}
toy <- expand.grid(x = 1:3, y = 1:3)
toy$f <- c(.16, .16, .08, .16, .16, .08, .08, .08, .04)

toy %>%
  spread(x, f) %>%
  arrange(desc(y)) %>%
  rename(`$y\\backslash x$` = y) -> toy_dsn

toy_dsn$`$f_Y(y)$` <- rowSums(toy_dsn[, -1])
toy_dsn[nrow(toy_dsn) + 1, -1] <- colSums(toy_dsn[, -1])
toy_dsn[nrow(toy_dsn), 1] <- "$f_X(x)$"

toy_dsn %>%
  xtable(align = paste0(paste(rep("c", ncol(.)), collapse = ""), "|c")) %>%
  print(include.rownames = FALSE, comment = FALSE, 
        sanitize.text.function = function(x){x},
        hline.after = c(-1, 0, nrow(.) - 1, nrow(.)))

```

**What do you notice?**

\vfill

\begin{df}
Discrete random variables $X$ and $Y$ are \emph{independent} if their joint distribution function $f(x, y)$ is the product of their respective marginal probability functions. This is, independence means that
$$
f(x, y) = f_X(x)f_Y(y) \qquad \text{for all } x, y.
$$
If this does not hold, then $X$ and $Y$ are \emph{dependent.}
\end{df}

**Alternatively**, discrete random variables $X$ and $Y$ are independent if for all $x$ and $y$,

\vspace{.5in}

If $X$ and$Y$ are not only independent but also have the same marginal distribution, then they are **independent and identically distributed (iid)**.

\newpage

## Functions of several random variables

We've now talked about ways to simultaneously model several random variables. An important engineering use of that material is in the analysis of system output that are functions of random inputs.

### Linear combinations

For engineering purposes, it often suffices to know the mean and variance for a function of several random variables, $U = g(X_1, X_2, \dots, X_n)$ (as opposed to knowing the whole distribution of $U$). When $g$ is **linear**, there are explicit functions.

\vspace{.2in}

\begin{prop}
\label{prop:linear}
If $X_1, X_2, \dots, X_n$ are $n$ independent random variables and $a_0, a_1, \dots, a_n$ are $n + 1$ constants, then the random variable $U = a_0 + a_1X_1 + a_2X_2 + \cdots + a_nX_n$ has mean
$$
\text{E}U = a_0 + a_1\text{E}X_1 + a_2\text{E}X_2 + \cdots + a_n\text{E}X_3
$$
and variance
$$
\text{Var}U = a_1^2\text{E}X_1 + a_2^2\text{E}X_2 + \cdots + a_n^2\text{E}X_3
$$
\end{prop}

\newpage

\begin{ex}
Say we have two independent random variables $X$ and $Y$ with $\text{E}X = 3.3, \text{Var}X = 1.91, \text{E}Y = 25$, and $\text{Var}Y = 65$. Find the mean and variance for 
\begin{align*}
U &= 3 + 2X - 3Y \\
V &= -4X + 3Y \\
W &= 2X - 5Y \\
Z &= -4X - 6Y
\end{align*}
\end{ex}

\newpage

\begin{ex}
Say $X \sim Binomial(n= 10,p= 0.5)$ and $Y\sim Poisson(\lambda= 3)$. Calculate the mean and variance of $Z = 5 + 2X - 7Y$.
\end{ex}

\newpage

A particularly important use of Proposition \ref{prop:linear} concerns $n$ iid random variables where each $a_i = \frac{1}{n}$. 

\vspace{1in}

We can find the mean and variance of the random variable

$$
\overline{X} = \frac{1}{n} X_1 + \cdots \frac{1}{n}X_n = \frac{1}{n}\sum\limits_{i=1}^n X_i
$$

as they relate to the population parameters $\mu = \text{E}X_i$ and $\sigma^2 = \text{Var}X_i$.

For independent variables $X_1, \dots, X_n$ with common mean $\mu$ and variance $\sigma^2$,

$\text{E}\overline{X} =$

\vfill

$\text{Var}\overline{X} =$

\vfill
 
\newpage 

\begin{ex}[Seed lengths]
One botanist measured the length of $10$ seeds from the same plant. The seed lengths measurements are $X_1,X_2, \dots,X_{10}$. Suppose it is known that the seed lengths are iid with mean $\mu= 5$ mm and variance $\sigma^2= 2$mm.

Calculate the mean and variance of the average of $10$ seed measurements.
\end{ex}

\vspace{4in}

### Central limit theorem

One of the most frequently used statistics in engineering applications is the sample mean. We can relate the mean and variance of the probability distribution of the sample mean to those of a single observation when an iid model is appropriate.

\newpage

\begin{prop}
If $X_1, \dots, X_n$ are iid random variable (with mean $\mu$ and variance $\sigma^2$), then for large $n$, the variable $\overline{X}$ is approximately normally distributed. That is,

$$
\overline{X} \stackrel{\cdot}{\sim} Normal\left(\mu, \frac{\sigma^2}{n}\right)
$$
\end{prop}

This is one of the **most important** results in statistics.

\vspace{0.2in}

\begin{ex}[Tool serial numbers]
Consider selecting the last digit of randomly selected serial numbers of pneumatic tools. Let

\begin{align*}
W_1 &= \text{ the last digit of the serial number observed next Monday at 9am} \\
W_2 &= \text{ the last digit of the serial number observed the following Monday at 9am}
\end{align*}
\end{ex}

A plausible model for the pair of random variables $W_1, W_2$ is that they are independent, each with the marginal probability function 

$$
f(w) = \begin{cases} .1 & w = 0, 1, 2, \dots, 9 \\ 0 & \text{otherwise}\end{cases}
$$

```{r}
w <- 0:9
f <- rep(1/length(w), length(w))

ggplot() + 
  geom_bar(aes(w, f), stat = "identity") +
  ylab("f(w)")

```

With $\text{E}W = 4.5$ and $\text{Var}W = 8.25$.

\newpage

Using such a distribution, it is possible to see that $\overline{W} = \frac{1}{2}(W_1 + W_2)$ has probability distribution

```{r, results='asis'}
w1 <- w2 <- 0:9
probs <- matrix(rep(0, length(w1)^2), nrow = length(w1))

for(i in seq_along(w1)) {
  for(j in seq_along(w2)) {
    probs[i, j] <- f[i]*f[j]
  }
}

expand.grid(w1 = w1, w2 = w2) %>%
  mutate(w_bar = .5*(w1 + w2),
         f = as.numeric(probs)) %>%
  group_by(w_bar) %>%
  summarise(f = sum(f)) -> w_bar_dsn

w_bar_dsn %>%
  rename(`$\\overline{w}$` = w_bar,
         `$f(\\overline{w})$` = f) -> w_bar_dsn_tab

cbind(w_bar_dsn_tab[1:4,], w_bar_dsn_tab[5:8,], w_bar_dsn_tab[9:12,], w_bar_dsn_tab[13:16,], rbind(w_bar_dsn_tab[17:19,], c("", ""))) %>%
  xtable(align = "ccc|cc|cc|cc|cc") %>%
  print(include.rownames = FALSE,
        comment = FALSE,
        sanitize.text.function = function(x){x})
```

```{r}
w_bar_dsn %>%
  ggplot() +
  geom_bar(aes(w_bar, f), stat = "identity") +
  xlab(expression(bar(w))) + ylab(expression(f(bar(w))))

```

```{r, cache = TRUE}
w <- 0:9

library(gganimate)

get_means <- function(i) {
  samps <- matrix(sample(w, size = 1000*i, replace = TRUE), nrow = 1000)
  apply(samps, 1, mean)
}

data.frame(iter = 1:40) %>%
  group_by(iter) %>%
  do(data.frame(means = get_means(.$iter))) -> clt


data.frame(iter = 1:40) %>%
  group_by(iter) %>%
  do(data.frame(x = seq(0, 9, length.out = 200))) %>%
  mutate(f = dnorm(x, 4.5, sqrt(8.25/iter))) -> norm_approx


p <- ggplot(clt, aes(frame = factor(iter))) +
  geom_density(aes(means), colour = "blue", fill = "blue", alpha = 0.1) +
  geom_line(aes(x, f), data = norm_approx)


gganimate(p, filename = "images/clt_animate.mp4", interval = .2)
```

Comparing the two distributions, it is clear that even for a completely flat/uniform distribution of $W$ and a small sample size of $n = 2$, the probability distribution of $W$ looks more bell-shaped that the underlying distribution.

Now consider larger and larger sample sizes, $n = 1, \dots, 40$:

\begin{center}
\begin{figure}[ht]
\includemovie[poster, text={Click for video...}]{6in}{3in}{images/clt_animate.mp4}
\end{figure}
\end{center}

\newpage

\begin{ex}[Stamp sale time]
Imagine you are a stamp salesperson (on eBay). Consider the time required to complete a stamp sale as $S$, and let 
$$
\overline{S} = \text{the sample mean time required to complete the next 100 sales}
$$
Each individual sale time should have an $Exp(\alpha= 16.5 s)$ distribution. We want to consider approximating $P[\overline{S} > 17]$. 
\end{ex}

\newpage

\begin{ex}[Cars]
Suppose a bunch of cars pass through certain stretch of road. Whenever a car comes, you look at your watch and record the time. Let $X_i$ be the time (in minutes) between when the $i^{th}$ car comes and the $(i+ 1)^{th}$ car comes for $i= 1,\dots,44$. Suppose you know the average time between cars is $1$ minute. Find the probability that the average time gap between cars exceeds 1.5 minutes.
\end{ex}

\newpage

\begin{ex}[Baby food jars, cont'd]
The process of filling food containers appears to have an inherent standard deviation of measured fill weights on the order of $1.6g$. Suppose we want to calibrate the filling machine by setting an adjustment knob and filling a run of $n$ jars. Their sample mean net contents will serve as an indication of the process mean fill level corresponding to that knob setting.

You want to choose a sample size, $n$, large enough that there is an $80$% chance the sample mean is within $.3$g of the actual process mean. 
\end{ex}

\newpage

\begin{ex}[Printing mistakes]
Suppose the number of printing mistakes on a page follows some unknown distribution with a mean of $4$ and a variance of $9$. Assume that number of printing mistakes on a printed page are iid.

\begin{enumerate}
\item What is the approximate probability distribution of the average number of printing mistakes on 50 pages?
\vfill
\item Can you find the probability that the number of printing mistakes on a single page is less than 3.8?
\vfill
\item Can you find the probability that the average number of printing mistakes on 10 pages is less than 3.8?
\vfill
\item Can you find the probability that the average number of printing mistakes on 50 pages is less than 3.8?
\vfill
\end{enumerate}
\end{ex}




